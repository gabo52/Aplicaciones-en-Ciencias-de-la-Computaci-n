# Agentes inteligentes

## Agentes y ambientes

Un **agente** es todo aquello que puede ver y percibir su **ambiente** mediante **sensores** y actuar  en su ambiente mediante actuadores. Un humano tiene ojos, oidos y otros organos como sensores y tiene boca, brazos, manos como actuadores. Un agente de software recibe pulsaciones de tecla, contenido de archivos, paquetes de internet como inputs sensoriales y actuan en el ambiente mostrando en pantalla escritos y enviando paquetes de internet.

Usamos el termino de **percibir** para referirnos a las percepciones de input de un agente en todo instante. Cuando una secuencia de percepción de un agente es completada, el agente ya ha percibido.  En general, una elección de accion de un agente en cualquier instante depende de la secuencia de percepción observada hasta la fecha, pero no de algo que todavía no ha sido percibido. Mediante la especificación de una acción de un agente para toda posible secuencia de percepciones, hemos dicho más o menos lo que hay que decir sobre el agente. Matematicamente hablando, podemos decir que el comportamiento de un agente está descrito por la **funcion del agente** que mapea toda secuencia de percepción recibida a una acción.
[imagen1]
Nos podemos imaginar tabulando la función del agente que describa todo agent; para la mayoría de los agentes, esto sería demasiado tedioso y largo(infinito), a menos que pongamos un limite en la longitud de la secuencia de percepciones que queremos considerar. Teniendo un agente para experimentar con el, podemos construir una tabla intentando mapear todas las posibles secuencias de percepciones y recordando qué acciones   el agente hace en respuesta. La tabla es una caracterización externa del agente. Internamente la función del agente para el agente artificial, debe ser implementada como un **programa de agente**. Es importante mantener estas 2 ideas distintas. La función del agente es una descripción matemática abstracta, el programa del agente es una implementación concreta, ejecutandose en algún sistema físico.

Para ilustrar estas ideas, usaremos un ejemplo bastante simple. Vamos a tener un pequeño mundo para una aspiradora. En este mundo podemos describir todo lo que sucede, es un mundo inventado por lo que podemos crear muchas variantes. Este mundo particular tiene 2 posiciones, el cuadrado A y el cuadrado B. El agente aspiradora percibe en qué posición se encuentra y qué posiciones están sucias. Se puede mover a la derecha, a la izquierda, limpiar la suciedad o no hacer nada. Una acción simple de la función agente es la siguiente: si la actual posición está sucia, luego limpia, de otra forma se mueve a otra posición.

[imagen2]

En la figura 3 vemos varios agentes aspiradora que pueden ser definidos rellenando la columna de la derecha de varias maneras. La pregunta obvia es: ¿Cuál es el correcto camino para llenar la tabla? En otras palabras que hace a un agente bueno o malo? 

[imagen3]

Antes de cerrar esta sección vamos a enfatizar la noción de un agente es ser una herramienta para analizar los sistemas, no es una caracterización absoluta que divide al mundo en agentes y no agentes. Uno puede ver a una calculadora de mano como un agente que escoge la acción de mostrar "4" cuando percibe la secuencia de "2 + 2 = " pero tal análisis difícilmente ayudaría a comprenderlo. En este sentido, todas las áreas de ingeniería pueden ser vistas como artefactos de diseño que interactuan con el mundo. AI opera como(segun los autores consideran) el extremo más interesante del espectrol, donde los artefactos tiene un recursos computacionales significativos y la tarea del ambiente requiere realizar decisiones no triviales.

## Buen comportamiento: El concepto de racionalidad

Un **agente racional** es aquel que hace las cosas correctas. Obviamente, hacer las cosas correctas es mejor que hacerlas incorrectas, pero qué significa hacer las cosas correctas? 

Respondemos esta pregunta antigua de una forma antigua: si consideremos las consecuencias del comportamiento del agente. Cuando un agente se coloca en un ambiente, este genera una secuencia de acciones de acuerdo a las percepciones que recibe. Esta secuencia de acciones provoca al ambiente a ir por una secuencia de estados. Si la secuencia es deseable, el agente ha efectuado correctamente. La noción de deseable es capturada por la **medida del rendimiento** que evalúa todas las demás todas las secuencias de los estados del ambiente.

Notemas que decimos **estado del ambiente**, y no **estado del agente**. Esto define la realización en términos de la opinión del agente y no de su performance. Un agente puede lograr racionalidad perfecta simplemente engañándose a si mismo diciéndose que su rendimiento fue perfecto.

Obviamente, no hay una medida de rendimiento fja para todas las tareas y agentes, el diseñador va a idear uno apropiado para cada circunstancia. No es tán fácil como suena. Consideremos, por ejemplo, a la aspiradora de limpieza de la sección anterior. Podemos proponer una medida de rendimiento como la cantidad de suciedad que limpia un horario de ocho horas. Como un agente racional, por su puesto, lo que preguntas es lo que tienes. Un agente racional puede maximizar su performance limpiando la suciedad y luego botandola en el piso, luego limpiandola de nuevo y así.  Una mejor medida de rendimiento puede recompensar al agente por tener el piso limpio. Por ejemplo, un punto puede ser recomensar por cada cuadrado limpio en cada paso de tiempo (quizaś con una penalización de por consumo de electricidad y ruido generado). Como regla general, es mejor diseñar una medida de rendimiento de acuerdo a lo que se desea en el ambiente, en vez de diseñarla de acuerdo a como quiere uno que el agente se comporte.

Siempre y cuando se eviten los errores principales, habrán cuestiones complicadas que resolver. Por ejemplo, la noción de "piso limpio" en el anterio párrafo está basado en el promedio de limpieza en el tiempo. Este promedio de limpieza puede ser abarcado por 2 diferentes agentes. Uno el cual realice un trabajo mediocre todo el tiempo y otro que limpie enérgicamente pero tome pausas largas. 

### Racionalidad 

Lo que es racional en todo tiempo depende de 4 cosas:

- La medida de rendimiento que define los criterios a alcanzar.
- El conocimiento previo del agente sobre su ambiente.
- Las acciones que el agente puede realizar.
- La secuencia de percepción del agente hasta la fecha.

Esto nos lleva a la definición de un agente racional:

*para cada posible secuencia de percepción del agente, un agente racional debe seleccionar una acción que espera máximizar la medida de rendimiento, dada las pruebas aportadas por la secuencia de percepción y todos los conocimientos integrados que el agente tiene.*

Considerando al agente aspiradora de limpieza que limpia un cuadrado si está sucio y se mueve al otro cuadrado sino. Este agente es racional? Depende, primero debemos ver la medida de rendimiento, qué se sabe acerca del ambiente, y que sensores y actuadores tiene el agente. Asumamos lo siguiente:

- La medida de rendimiento gana un punto por cada cuadrado limpio en cada paso del tiempo durante toda una vida de 10000 pasos del tiempo.
- La "geografía" del entorno se sabe a priori(figura 2.2), pero la distribución de la suciedad y la posición inicial del agente, no. Las plazas limpias permanecen limpias y aspirar limpia la posición actual. Las acciones *left* y *right* mueven al agente a la izquierda y a la derecha excepto cuando esto saque al agente de su entorno, en este caso el agente permanece donde está.
- Las únicas acciones válidas son *left*, *right* y *suck*.
- El agente percibe correctamente su posición y si esa posición está sucia.
  
Podemos decir que bajo estas circunstancias el agente en efecto es racional, este rendimiento esperado es al menos tan elevado como cualquier otro agente. 

Uno puede ver fácilmente que el mismo agente puede volver irracional bajo distintas circunstancias. Por ejemplo, una vez toda la suciedad se haya limpiado, el agente puede encontrarse oscilando innecesariamente de un lado a otro, esta medida de rendimiento incluye una penalidad por cada movimiento left o right que realice, por lo que el agente bajará su rendimiento. Un mejor agente para este caso no haría nada una vez se haya limpiado toda la suciedad. SI el cuadrado limpio se vuelve sucio de nuevo, el agente debe ocasinalmente revisar y volver a limpiar si es necesario. Si la geografía del entorno es incierta, el agente debe explorar en lugar de quedarse en su posición.


### Omniscience, learning, and autonomy

Debemos ser cuidadosos con distinguir entre racionalidad y *omnisciencia*. Un agente omnisciente sabe el resultado real de sus actos y puede actuar de acuerdo a ello, pero la omnisciencia es imposible, en realidad. Consideremos el siguiente caso: Estoy caminando a lo largo de Champs Elysées un día y veo a un amigo cruzando la calle. No hay tráfico cerca y nos comprometido para realizar alguna actidad en ese momento, entonces siendo racional, comienzo a cruzar la calle. Mientras tanto a 33 000 pies de altura, se cae una puerta de carga en una aerolínea de pasajeros y antes de llegar al otro lado de la calle, estoy aplastado. Fue irracional cruzar la calle? No.

Este ejemplo demuestra que la racionalidad no es lo mismo que perfección. La racionalidad maximiza el rendimiento *esperado*, mientras que la perfeccion maximiza el rendimiento *actual*. Retirarse de un requirimiento de perfección no solo es una cuestión de ser justos con los agentes. El punton es que si nosotros esperamos  a que un agente haga lo que resulte ser la mejor acción después de un hecho, va a ser imposible diseñar un agente para realizar esta especificación. 

Nuestra definición de racionalidad no requiere omnisciencia, porque la opcion racional depende solo de  la secuencia de percepciones hasta la fecha. Debemos asegurarnos que no hemos permitido inadvertidamente que el agente se dedique a actividades decididamente poco inteligentes. Por ejemplo, si un agente no mira ambos lados de la calle antes de cruzar la calle, su secuencia de percepciones no va a decirle que un camión se acerca a alta velocidad. Nuestra definición de racionalidad dice que está bien cruzar la calle ahora? Ni mucho menos. Primero, no sería racional cruzar la calle teniendo una secuencia de percepciones con falta de información: el riesgo de cruzar la calle sin ver a ambos lados es bastante grande. Segundo, un agente racional debe escoger la accion de *mirar* antes de cruzar la calle, dado que ver a ambos lados de la calle maximiza el rendimiento esperado. Realizar acciones *para modificar las percepciones futuras* (aveces llamado **recopilación de información**) es una parte importante de la racionalidad. Un segundo ejemplo de recopilación de información es provista por la **exploracion** que debe realizar la aspiradora de limpieza en un entorno desconocido. 

Nuestra definición requiere un agente racional no solo para recoger información, sino para **aprender** tanto como es posible de lo que perciba. La configuración del agente inicial puede reflejar conocimiento previo del entorno, pero conforme el agente gane experiencia, este puede ser modificado y puede ser aumentado. Hay casos extremos donde el entorno es completamente conocido a priori. En estos casos, el agente no necesita aprender o percibir, solo debe actuar correctamente. Por supuesto, algunos agentes son frágiles. Consideremos el humilde escarabajo pelotero. Luego de cavar su nido y poner sus huevos, este toma una pelota de un montón cercano para tapar la entrada. Si la bola de estiércol se le escapa por el camino, el escarabajo continúa su tarea y hace la pantomima de taponar el nido con la inexistente bola de estiércol, sin darse cuenta de que le falta. La sphex hembra excavará una madriguera, saldrá a picar a una oruga y la arrastrará hasta la madriguera, entrará de nuevo en la madriguera para comprobar que todo va bien, arrastrará a la oruga al interior y pondrá sus huevos. La oruga sirve como fuente de alimento cuando los huevos eclosionan. hasta aquí todo bien, pero si un entomólogo aleja la oruga unos centímetros mientras el sphex está haciendo la comprobación, volverá al paso "arrastrar" de su plan y continuará el plan sin modificaciones, incluso después de docenas de intervenciones para mover la oruga. El sphex es incapaz de aprender que su plan innato está fallando, y por lo tanto no lo cambiará.

Para entender que un agente confía en el conocimiento previo de su diseñador y no en sus propias percepciones, vamos a decir que el agente es poco **autonomo**. Un agente racional debe ser autonomo, este debe aprender lo que pueda para compensar un conocimiento previo parcial o incorrecto. Por ejemplo, un agente como la aspiradora de limpieza, que aprenda a preveer donde y cuando aparecerá basura adicional va a trabajar mejor un agente similar que no haga esto. En asuntos prácticos, rara vez se requiere una autonomía total desde el principio: cuando un agente tiene poca o nula experiencia, este debe actuar de forma aleatoria a menos que el diseñador le de alguna asistencia. Por lo que sería razonable proveer a un agente de inteligencia artificial, conocimiento inicial y la habilidad para aprender. Después de suficiente experiencia en el entorno, el comportamiento del agente racional puede convertirse en efecivamente **independiente** de sus conocimientos previos.  Por ende, la incorporación de aprendizaje permite a uno diseñar un agente racional singular que tendrá exito en una gran variedad de entornos.

Ahora que sabemos la definición de racionalidad, estamos listos para entender sobre la construcción de agentes racionales. Primero, debemos pensar sobre las **tareas de entorno**, los cuales son los esenciales "problemas" para el cual los agentes racionales son la **solución**. Comenzamos mostrando como especificar una tarea de entorno, ilustrando el proceso con un número de ejemplos. Luego mostramos que estas tareas de entorno, vienen en una variedad de sabores. El sabor de la tarea de entorno afecta directamente al diseño apropiado del programa del agente.

### Specifying the task environment

En nuestra discusión de la racionalidad de un simple agente como aspiradora de limpieza, tenemos que especificar la medida de rendimiento, el entorno y los actuadores y sensores del agente. Agrupamos todo esto bajo el título de **tareas de entorno**. Para los amantes de las siglas, llamamos esto como el PEAS(Performance, Environmente, Actuators, Sensors) descriptor. En el diseño como agente, el primer paso siempre debe ser espeficiar la tarea de entorno lo más completamente posible.

El mundo de la aspiradora fue un ejemplo simple, consideremos uno más complejo: un taxi que maneja automáticamente. Debemos señalar algo, antes que el lector se alarme, un taxi completamente automatizado es algo ir más allá de las capacidades de la tecnología existente.  Las tareas de este taxi son extremedamente abiertas. No hay límite para la novela de combinaciones de circunstancias a las que puede llevar. La figura 2.4 resume la descripción PEAS para las tareas de entorno del taxi. Vamos a discutir cada elemento en los siguientes párrafos.

[imagen2.4]

Primero, cuál es el medidor de rendimiento al cual queremos que el conductor autómatico aspire? Cualidades deseable incluyen llegar al destino correcto, minimizar el consumo de gasolinna por desgeste, minimizar el costo del viaje, minimizar las violaciones de leyes de tránsito y disturbios con otros conductores, maximizar el confort y la seguridad, maximizar las ganancias. Obviamente algunos de los objetivos tienen conflictos, por lo que se necesitan tradeoffs.

Luego, cuál es el entorno de conducción al cual el taxi se va a enfrentar? Los taxistas deben encontrarse con una variedad de carreteras,  que van desde carreteras rurales, urbanas y autopistas. Las carreteras contienen tráfico, peatones, animales vagabundos, trabajos en la pista, carros de policia, charcos y baches. El taxi debe interactuar con potenciales y pasajeros actuales. De hecho hay algunas opciones extra. El taxi puede necesitar operar en el sur de California, donde la nieve rara vez es un problema, o en Alaska, donde rara vez no lo es. Puede estar siempre manejando a la derecha, o puede estar manejando a la izquierda si estamos en Inglaterra o Japón. Obviamente, el entorno mas restrictivo, es el mas fácil para el diseño del problema. 

Los **actuadores** para el taxi automático incluyen aquellas disponibles para el conductor humano: control del motor a través del acelerador y control de la dirección y el frenado. Además, este necesitará mostrar una pantalla o un sintetizador de voz para hablar con los pasajeros, y quizás alguna forma de comunicarse con otros vehículos, educadamente o no.

Los sensores **básicos** para el taxi incluyen una o más cámaras controlables que puedan ver la pista, podría aumentarlos con sensores de infrarrojos o de para detectar distancias a otros coches y obstáculos. Para evitar multas por exceso de velocidad, especialmente en curvas, este debe tener un acelerómetro. Para determinar el estado mecánico del vehículo, necesitará la matriz habitual del motor, gasolina, y sensores del sistema eléctrico. Como los conductores humanos, este puede necesitar un GPS para que no se pierda. Finalmente, este necesitaría un teclado o micrófono para que el pasajero pueda solicitar un destino.

En la figura 2.5, vemos los elementos básico del PEAS para un número de tipos de agente adicionales. Puede ser sorpresa para algunos lectores que nuestra lista de agentes incluye algunos programas que operan en un entorno enteramente artificial, definido por el input del teclado y el output mostrado en la pantalla. Uno puede decir "este no es un enterno real, o si lo es?" De hecho, lo que importa no es la distinción entre un entorno "real" y "artificial", pero si la complejidad de la relación entre el comportamiento del agente, la secuencia de percepción generada por el entorno y la medida de rendimiento. Algunos entornos "reales" son actualmente bastante simples. Por ejemplo, un robot diseñado para inspeccionar piezas a medida que pasan por una cinta transportadora puede hacer uso de una serie de suposiciones simplificadoras: que el rayo siempre está justo así, que lo único que habrá en la cinta transportadora serán piezas de un tipo que conoce y que sólo son posibles dos acciones (aceptar o rechazar).

En contraste, algunos **agentes de software**(o robots de software o **softbots**) existen en dominios ricos e ilimitados. Imaginemos a un softbot que opere en un sitio web, este está diseñado para escanear nuevos recursos de internet y mostrarle los más interesantes al usuario y vender espacios publicitarios para generar ingresos. Para realizarlo bien, ese operador necesitará ciertas capacidades de procesamiento del lenguaje natural, tendrá que aprender lo que interesa a cada usuario y anunciante, y deberá cambiar sus planes dinámicamente-por ejemplo, cuando se cae la conexión de una de las nuevas fuentes o cuando se conecta una nueva. El internet es un entorno cuya complejidad rivaliza con la del mundo físico y cuyos habitantes incluyen numerosos agentes artificiales y humanos.

### 2.3.2 Properties of task environments

La gama de entornos de tareas que pueden surgir en la IA es obviamente muy amplia. Podemos, de todos modos, identificar un número bastante reducido de dimensaiones a lo largo cuyo entorno de tareas puede categorizarse. Estas dimensiones determinan, en gran medida, el diseño aprodiado del agente y la aplicabilidad de cada una de las principales familias de técnicas para la implantación de agentes. Primero, vamos a listar las dimensiones, luego vamos a analizar las tareas de entorno para ilustrar las ideas. Las definiciones acá son informales.

**Fully observable** vs **partially observable**: Si los sensores del agente tiene acceso a los estados completos del entorno en cada punto del tiempo, podemos decir que el entorno de tareas es completamente observable. Una tarea de entorno es efectivamente totalmente observable si los sensores detectan todos los aspectos que son *relevantes* para escoger alguna acción, relevancia, a su vez, depende de la medida de rendimiento. Entornos completamente observables son convenientes dadoo que el agente no necesita mantener estados internos para no perder del vista el mundo.  Un entorno puede ser parcialmente observable por el ruido y los sensores inprecisos o porque partes del estado simplemente se pierden de los datos del sensor-por ejemplo, el agente aspiradora con un solo sensor local no puede decir donde está sucio en otros cuadrados, y un taxi automatico no puede saber que están pensando los otros conductores. Si el agente no tiene sensores, el entorno se convierte en inobservable. Uno puede pensar que en estos casos que la situación de los agentes no tiene remedio, pero como veremos en el capitulo 4, los objetivos del agente aún pueden lograrse, aveces con cierta certeza.

**Single agent** vs **multiagent**. La diferencia entre un entorno de agente singular y uno multiagente parece ser suficientemente simple. Por ejemplo, un agente resolviendo un crucigrama por sí mismo es claramente un entorno de un único agente, mientras que un agente jugando ajedrez es un entorno de 2 agentes. Ahí hay de todas formas, algunos errores sutiles. Primero, hemos descrito como una entidad *puede* verse com un agente, pero no hemos explicado qué entidades deben verse como agentes. Un agente A(el taxista, por ejemplo) tiene que lidiar con el objeto B(otro vehículo)como agente, o puede lidiar como un objeto que se comporta de acuerdo con las leyes de la física, análogo a las olas en el mar o soltar globos en el aire? La distinción clave es si el comportamiento de B se describe mejor como una maximización  de una medida de rendimiento cuyos valores dependen de los comportamiento del agente A. Por ejemplo, en ajedrez, la entidad de oponente B trata de mejorar su medida de rendimiento, donde, bajo las reglas del ajedrez, minimiza las medidas de rendimiento del agente A. Por lo tanto, el ajedrez es un juego  de entorno multiagente **competitivo**. En el entorno del taxista, por otro lado, evitar colisionar maximiza la medida de rendimiento de todos los agentes, entonces, esta es parcialmente un entorno **cooperativo** multiagente. Este es parcialmente competitivo dado que, por ejemplo, solo un carro puede ocupar un espacio de parqueo. Los problemas de diseño de agentes en un entorno multiagente suelen ser muy diferentes de los que se plantean en entornos con un único agente. Veamos el siguiente caso: la **comunicacion** usualmente surge como un comportamiento racional en entornos multiagente, en algunos entornos competitivos, **randomized behavior** es racional dado a que estos evitan las dificultades de predicción.

**Deterministic** vs **stochastic**. Si el siguiente estado del entorno está completamente definido por el estado actual y la acción ejecutada por el agente, podemos decir que el entorno es deterministico; de otra forma es estocástico. En el primero, el agente no necesita preocuparse acerca de la incertidumbre en un entorno determinístico y completamente observable(en nuestra definición, nosotros ignoramos la incertidumbre que surge únicamente por las acciones de otros agentes en un entorno multiagente, entonces, un juego puede ser determinístico siempre y cuando cada agente esté imposibilitado de predecir las acciones de otros). Si el entorno es parcialmente observable, este puede *parecer* ser estocástico. La mayor de las situaciones reales son tan complejas que es imposible mantener la cuenta de todos los aspectos inobservables, para propósitos prácticos, este será tratado como estocástico. El caso del taxi es claramente estocástico en este sentido, dado que uno nunca va a poder predecir el comportamiento exacto del tráfico, además un neumático puede reventar un motor se puede agarrotar sin previo aviso. El mundo de la aspiradora que hemos descrito es determinístico, pero algunas variaciones pueden incluir elementos estocásticos como suciedad que aparece repentinamente y un mecanismo de succión poco fiable. Decimos que un ambiente es **uncertain** si no es completamente observable o no es determinístico. Una nota fina: el nuestro uso de la palabra "estocástico" generalmente implica incertidumbre acerca de resultados qu se cuantifican en términos de probabilidad, un entorno **no deterministico** es uno el cual las acciones están caracterizadas por su *posible* resultado, pero las probabilidades no están de su lado. Las descripciones de los entornos no-deterministicos están usualmente asociados con las medidas de rendimiento que requiere el agente para salir victorioso en todos los posibles resultados de sus acciones.

**Episodic** vs **sequential**: En una tarea de entorno por episodios, la experiencia del agente está divididad en episodios atómicos. En cada episodio el agente recibe una percepción y luego logra una única acción.  Crucialmente, el siguiente episodio no depende de las acciones tomadas en los previos eposidios. Muchas clasificaciones de tareas son episódicas. Por ejemplo, un agente que tiene que identificar partes defectuases de una cadena de montaje basa cada decisión en la pieza actual independientemente de la decisión previa, además, la decisión actual no afecta si la siguiente parte está defectuosa. En entornos secuenciales, por otro lado, la decisión actual puede afectar a todas las decisiones futuras. El ajedrez y el taxi son secuenciales: en ambos casos, acciones a corto plazo puede tener consecuencias a largo plazo. Los entornos por episodio son mucho más simples que los entornos secuenciales dado que el agente no tiene que pensar en el futuro.

**Static vs dynamic**: Si el entorno puede cambiar mientras el agente está deliberando, entonces decimos que el entorno es dinámico para el agente, de otra forma, es estático. Entornos estáticos son faciles de enfrentar dado que el agente no necesita estár viendo el mundo mientras decide una acción ni necesita preocuparse por el paso del tiempo. Los entornos dinámicas, por otro lado, preguntan continuamente al agente qué quiere hacer; si no lo ha decidido aún, esto cuenta como decidir no hacer nada. Si el entorno mismo no cambia con el paso del tiempo, pero el rendimiento del agente lo hace, podemos decir que el ambiente es **semidynamic**. El caso del taxi es claramente dinámico: los otros carros y el taxi se mantienen moviéndose mientras el algoritmo de manejo titubea qué hacer después. En el ajedrez cuando juegan con un reloj, es semidinámico. Los Crucigramas son estáticos.

**Discrete** vs **continuous**: La distinción de discreto/continuo aplica al *estado* del entorno, a la forma en que se maneja el tiempo, y como se perciben las acciones del agente. Por ejemplo, el entorno del ajedrez tiene un número finito de distintos estados(excluyendo el reloj). El ajedrez entonces puede ser un grupo discreto de percepciones y acciones. El taxi es un estado-continuo y el problema de tiempo.continuo: la velocidad y la posición del taxi y los otros vehículos barren a traves de un rango de valores continuos y lo hacen lentamente a lo largo del tiempo. Las acciones del manejo del taxi son continuas(ángulos de direccion, etc). El input para las cámaras digitales es discreto hablando estrictamente, pero se suele considerar que representan una intensidad y una localización que varían constantemente.

**Known vs unknown**: Hablando estricatemente, esta distinción no se refiere al entorno en sí, pero si al estado de conocimiento del agente(o diseñador) acerca de las "leyes de la física" del entorno. En un entorno conocido, los resultados(o las probabilidades de resultado en un entorno estocástico) de todas las acciones se saben. Obviamente, si el entorno es desconocido, el agente tiene que aprender de este para poder tomar buenas decisiones. Nótese que la distinción entre entorno conocido y desconocido no es la misma entre que entre entorno completamente y parcialmente observables, por ejemplo, en las cartas de solitario, yo conozco las reglas, pero estoy imposibilitado de ver las cartas que aún no se han volteado. Por el contrario, un entorno desconocido puede ser completamente observable- en un nuevo videojuego, la pantalla puede no ser el estado completo del juego, pero aún no sé qué hacen los botones hasta que pruebe con ellos.

Como uno puede esperar, el caso más complicado es *partially observable, multiagente, stochastic, sequential, dynamic, continuos and unknown*. El caso del taxi es el más difícil en todos estos sentidos excepto por el hecho de que la mayoría del entorno es conocida. Manejar un carro rentado en un nuevo país, con una geografía desconocida y leyes de tránsito es más mucho más emocionante.

La figura 2.6 lista las propiedades de un número de entornos familiares. Nótese que las respuestas no siempre son sencillas. Por ejemplo, describimos el robot de recogida de piezas como un episodic, porque normalmente considera cada parte en la insolación. Pero si un día hay un gran lote de piezas defectuosas, el robot debe aprender con varias observaciones qué distribución o defectos ha cambiado y debe modificar este comportamiento para las partes subsiguientes. No incluimos una columna "conocido/desconocido" porque, como explicamos anteriormente, esta no es estrictamente una propiedad del entorno. Para algunos entornos, como el ajedrez o el poker, es bastante fácil suministrar al agente todo el conocimiento de las reglas, sin embargo, es interesante considerar como un agente puede aprender a jugar sin dichos conocimiento. 

[figura2.6]

Varias de las respuestas en la tabla depende de cómo son definidas las tareas del entorno. Tenemos listadas las tareas de diagnostico médico como un único agente dado que el proceso de la enfermadad en un paciente no es rentable modelarlo como un agente, pero un sistema de  diagnostico medico puede lidiar con pacientes recalcitrantes y personal escéptico, por lo que el entorno puede tener aspecto multiagente. Además, el diagnostico médico es episódico si uno concibe la tarea como seleccionar un diágnostico teniendo una lista de síntomas, el problema es secuencial si la tarea puede incluir la propuesta de una serie de pruebas, evaluando el progreso a lo largo del tratamiento y así. De todas formas, un entorno es episodico a niveles altos en relación a las acciones individuales del agente. Por ejemplo, un torneo de ajedrez consiste en una secuencia de juegos, cada juego es un episodio porque(a la larga) la contribución de los movimientos de una partida al rendimiento global de los agentes no se ve afectada por los movimientos en el juego previo. En la otra mano, decisiones hechas dentro de un mismo juego es ciertamente secuencial.

El repositorio de código asociado con este libro incluye implementaciones de un número de entornos, junto con un simulador de entorno de uso general que pone a uno o más agentes en un entorno simulado, observando su comportamiento en el tiempo y evaluándolo de acuerdo a las medidas de rendimiento. Estos experimiento a menudo no se llevan a cabo para un único entorno sino para muchos entornos extraídos de una **environment class**. Por ejemplo, para evaluar al taxi en un tráfico simulado, vamos a querer ejecutar varias simulaciones con distintos casos de tráfic, luces y condiciones de clima. Si diseñamos al agente para un único escenario, probablemente vamos a tener la posibilidad de tomar ventaja de propiedades específicas para casos particulares, pero quizás no podremos identificar un buen diseño de manejo en general. Por esta razón, el repositorio de código incluye un **environment generator**  para clase de entorno que selecciona entornos en particular(con cierta particularidad) en la que se ejecutará el agente. Por ejemplo, el generator de entorno para la aspiradora, inicializa los patrones suciedad y la localización del agente aleatoriamente. Estamos interesados en el promedio de rendimiento del agente, encima de la clase de entorno. Un agente racional para una clase de entorno dada maximiza su promedio de rendimiento.






Tipos de entornos:

- Fully observable
- partially observable
- Single agent
- multiagent



